{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8645eb02",
   "metadata": {},
   "source": [
    "# Chest X-Ray Abnormality Detection (Multi-Label CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67ed24",
   "metadata": {},
   "source": [
    "KENDALL MCNEIL\n",
    "\n",
    "November 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb1e43",
   "metadata": {},
   "source": [
    "insert image collage here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43250f4",
   "metadata": {},
   "source": [
    "DESCRIPTION:  The chest radiograph is one of the most challenging to interpret, which can result in misdiagnosis even for seasoned healthcare providers. Building a strong convolutional neural network (CNN) to detect common thoracic lung diseases in chest x-rays would improve diagnostic accuracy for patients and ultimately save lives through early and accurate detection. The CNN will act as an automated system to support radiologists as a second opinion in reviewing chest x-rays for abnormalities. The work product will alleviate the stress of busy doctors and healthcare providers while also providing patients with a more accurate and efficient diagnosis. \n",
    "\n",
    "OBJECTIVE: The objective, therefore, is to detect a variety (14 total) of common thoracic lung abnormalities in chest x-rays by building a Convolutional Neural Network (CNN) to develop an AI system for thoracic lung abnormality detection. The multi-label neural network model was designed using Tensorflow. \n",
    "\n",
    "AUDIENCES: The general target audience for the project is healthcare providers. The more specific presentation audience is Vingroup Big Data Institute (VinBigData) that is working to build large-scale and high-precision medical imaging solutions based on the latest advancements in AI to facilitate efficient clinical workflows. \n",
    "\n",
    "DATA: The dataset includes 18,000 dicom images and was created by assembling de-identified chest X-ray studies provided by two hospitals in Vietnam: the Hospital 108 and the Hanoi Medical University Hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2e890",
   "metadata": {},
   "source": [
    "# A. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a8edec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f786c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\Documents\\flatiron\\CAPSTONE\\data\n"
     ]
    }
   ],
   "source": [
    "cd Documents\\flatiron\\CAPSTONE\\data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b18d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "#tensorflow imports for CNN image classification project \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import regularizers, optimizers\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#additional plotting imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdd30c",
   "metadata": {},
   "source": [
    "First, let's read in the data. Then let's store the full dataset under \"original df\" then create a dataframe with only image and class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13612a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('train.csv')\n",
    "df = original_df[['image_id','class_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9470a",
   "metadata": {},
   "source": [
    "# B. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed26e3e",
   "metadata": {},
   "source": [
    "Let's create a legend for the class names and class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0090750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Class_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Aortic enlargement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Calcification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ILD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Infiltration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Lung Opacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nodule/Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Other lesion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Pleural effusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Pleural thickening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Pneumothorax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Pulmonary fibrosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>No finding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number          Class_Name\n",
       "0        0  Aortic enlargement\n",
       "1        1         Atelectasis\n",
       "2        2       Calcification\n",
       "3        3        Cardiomegaly\n",
       "4        4       Consolidation\n",
       "5        5                 ILD\n",
       "6        6        Infiltration\n",
       "7        7        Lung Opacity\n",
       "8        8         Nodule/Mass\n",
       "9        9        Other lesion\n",
       "10      10    Pleural effusion\n",
       "11      11  Pleural thickening\n",
       "12      12        Pneumothorax\n",
       "13      13  Pulmonary fibrosis\n",
       "14      14          No finding"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df = pd.DataFrame({'Number': list(range(15)),\n",
    "    'Class_Name': ['Aortic enlargement','Atelectasis','Calcification','Cardiomegaly','Consolidation','ILD',\n",
    "    'Infiltration','Lung Opacity','Nodule/Mass','Other lesion','Pleural effusion','Pleural thickening','Pneumothorax',\n",
    "    'Pulmonary fibrosis','No finding']})\n",
    "class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79f531",
   "metadata": {},
   "source": [
    "How many of each class are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74538d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_df = df['class_id'].value_counts()\n",
    "class_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8701c33",
   "metadata": {},
   "source": [
    "The distribution looks okay. There may be a class imbalance that may rear its head later. Let's peak into image ID value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd258064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_id'].value_counts().head(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac799cd",
   "metadata": {},
   "source": [
    "That's odd. Some image IDs have over 50 counts. Let's check on duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91649",
   "metadata": {},
   "source": [
    "Wow. There are a lot of duplicates. Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df['image_id'].value_counts().head(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa8e56",
   "metadata": {},
   "source": [
    "Much better. Let's also check that there are no duplicate photos in the images folder just to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac311ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'images'\n",
    "\n",
    "# Create a dictionary to store encountered file names.\n",
    "file_names = {}\n",
    "\n",
    "# Iterate through the files in the folder.\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a regular file (not a subdirectory).\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        # Check if the file name has been encountered before.\n",
    "        if filename in file_names:\n",
    "            print(f'Duplicate file name: {filename}')\n",
    "            print(f'First occurrence: {file_names[filename]}')\n",
    "            print(f'Second occurrence: {os.path.join(folder_path, filename)}')\n",
    "        else:\n",
    "            # Store the file name and its full path for future reference.\n",
    "            file_names[filename] = os.path.join(folder_path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9838cb",
   "metadata": {},
   "source": [
    "Great. There are no duplicate photos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c42798",
   "metadata": {},
   "source": [
    "# C. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef96ce",
   "metadata": {},
   "source": [
    "The next few cells are outdated as I was working to organize the data. I created a function that would create class id folders and copy images respectively into them, but then realized there are more efficient and storage-conscious ways to organize the data.\n",
    "\n",
    "Additionally, it is worth mentioning that the data provided did not have labeled targets (likely because it is an active competition). Therefore, we will use the 15,000 images in the train folder and take a subset of that set for testing. We will use 12500 images for training and 2500 images for testing - a 17/83% split. \n",
    "\n",
    "Originally, I had random.sample randomly select 2500 images and move them into the test folder, but later on decided to perform the split inside our ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44347fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create class folders and copy images\n",
    "def create_class_folders_and_copy_images(base_folder, class_counts, df):\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row['image_id']\n",
    "        class_id = row['class_id']\n",
    "\n",
    "        # Create a folder for the class if it doesn't exist\n",
    "        class_folder = os.path.join(base_folder, str(class_id))\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "        source_image_path = os.path.join(base_folder, f\"{image_id}.PNG\")\n",
    "        image_path = os.path.join(class_folder, f\"{image_id}.PNG\")\n",
    "\n",
    "        # Check if the source file exists before copying\n",
    "        if os.path.exists(source_image_path):\n",
    "            shutil.copy(source_image_path, image_path)\n",
    "            print(f\"Copied: {source_image_path} -> {image_path}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {source_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbd2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class folders and copy images for the \"train\" folder\n",
    "#create_class_folders_and_copy_images(train_folder, class_counts, df)\n",
    "\n",
    "# Create class folders and copy images for the \"test\" folder\n",
    "#create_class_folders_and_copy_images(test_folder, class_counts, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e5f4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_folder = 'train'\n",
    "#test_folder = 'test'\n",
    "\n",
    "# List all image files in the train folder\n",
    "#image_files = [file for file in os.listdir(train_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Randomly select 2500 images from the list\n",
    "#selected_images = random.sample(image_files, 2500)\n",
    "\n",
    "# Move the selected images to the test folder\n",
    "#for image in selected_images:\n",
    "    #source_path = os.path.join(train_folder, image)\n",
    "    #destination_path = os.path.join(test_folder, image)\n",
    "    #shutil.move(source_path, destination_path)\n",
    "\n",
    "#print(\"Randomly selected and moved 2500 images to the test folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea0520",
   "metadata": {},
   "source": [
    "We need to perform additional data preprocessing before using Tensorflow. Currently, there are multiple rows for each image that falls into multiple classes. To fix this for our ImageDataGenerator, we need to groupby class ID, one hot encode the class IDs, and then merge the dataframes. That will create a final dataframe that has one row per image with one hot encoded classes. Let's also shuffle the dataframe images just in case because I will be selecting the first 12500 for the test set and the remaining 2500 for the test set. Last, let's add \".png\" to all the image IDs so that our ImageDataGenerator can locate them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64b8041a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000434271f63a053c4128a0ba6352c7f</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00053190460d56c53cc3e57321387478</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0005e8e3701dfb1dd93d53e2ff537b6e</td>\n",
       "      <td>[7, 8, 6, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006e0a85696f6bb578e84fafa9a5607</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007d316f756b3fa0baea2ff514ce945</td>\n",
       "      <td>[13, 11, 3, 0, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>ffe6f9fe648a7ec29a50feb92d6c15a4</td>\n",
       "      <td>[3, 0, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>ffea246f04196af602c7dc123e5e48fc</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>ffeffc54594debf3716d6fcd2402a99f</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>fff0f82159f9083f3dd1f8967fc54f6a</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>fff2025e3c1d6970a8a6ee0404ac6940</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_id           class_id\n",
       "0      000434271f63a053c4128a0ba6352c7f               [14]\n",
       "1      00053190460d56c53cc3e57321387478               [14]\n",
       "2      0005e8e3701dfb1dd93d53e2ff537b6e       [7, 8, 6, 4]\n",
       "3      0006e0a85696f6bb578e84fafa9a5607               [14]\n",
       "4      0007d316f756b3fa0baea2ff514ce945  [13, 11, 3, 0, 5]\n",
       "...                                 ...                ...\n",
       "14995  ffe6f9fe648a7ec29a50feb92d6c15a4          [3, 0, 9]\n",
       "14996  ffea246f04196af602c7dc123e5e48fc               [14]\n",
       "14997  ffeffc54594debf3716d6fcd2402a99f                [0]\n",
       "14998  fff0f82159f9083f3dd1f8967fc54f6a               [14]\n",
       "14999  fff2025e3c1d6970a8a6ee0404ac6940               [14]\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: Group by image ID\n",
    "grouped = df.groupby('image_id')['class_id'].apply(list).reset_index()\n",
    "\n",
    "#Step 2: Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(grouped['class_id'].apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "#Step 3: Merge dataframes \n",
    "final_df = grouped.merge(one_hot_encoded, left_index=True, right_index=True)\n",
    "\n",
    "#Step 4: shuffle the dataframe\n",
    "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Step 5: add \".png\" to the image_ids \n",
    "final_df['image_id'] = final_df['image_id'] + '.png'\n",
    "\n",
    "#The final_df DataFrame now contains one-hot encoded class labels for each image ID with one row per image\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03886b44",
   "metadata": {},
   "source": [
    "Now we're cookin' and ready for Tensorflow's ImageDataGenerator!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905b059",
   "metadata": {},
   "source": [
    "# D. ImageDataGenerator Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b7c6ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 validated image filenames.\n",
      "Found 2500 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen= ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen= ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "final_columns = final_df.columns[2:].tolist()\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=final_df[:12500], directory='images', x_col='image_id',\n",
    "    y_col=final_columns, seed=42, class_mode='raw', color_mode='grayscale')\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "    dataframe=final_df[12500:], directory='images', x_col='image_id',\n",
    "    seed=42, class_mode=None, color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df480b",
   "metadata": {},
   "source": [
    "Let's also create a function to evaluate the models moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4cd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    train_loss, train_accuracy = model.evaluate(train_generator)\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    \n",
    "    print('----------')\n",
    "    \n",
    "    \n",
    "    print(f'Train Accuracy: {train_accuracy}')\n",
    "    print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d00624",
   "metadata": {},
   "source": [
    "# E. First Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12967ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first simple model\n",
    "fsmodel = keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 1)),  # Adjust input shape as needed\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(15, activation='sigmoid')  # Use 'sigmoid' for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "fsmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "fsmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model⏰ This cell may take about a minute to run\n",
    "fsmhistory = fsmodel.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(fsmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e39fe6",
   "metadata": {},
   "source": [
    "# F. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c968fc",
   "metadata": {},
   "source": [
    "This is our baseline first simple model that our model must perform better than. Let's start by adding some Conv2D and MaxPooling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f011c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 127, 127, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 246016)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               31490176  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15)                1935      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31510927 (120.20 MB)\n",
      "Trainable params: 31510927 (120.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(15, activation='sigmoid')  # Use 'sigmoid' activation for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ac2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 695s 2s/step - loss: 0.2114 - accuracy: 0.7481 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 674s 2s/step - loss: 0.1660 - accuracy: 0.7798 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 664s 2s/step - loss: 0.1448 - accuracy: 0.7890 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 686s 2s/step - loss: 0.1218 - accuracy: 0.8072 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 649s 2s/step - loss: 0.0912 - accuracy: 0.8188 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 666s 2s/step - loss: 0.0589 - accuracy: 0.8345 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 676s 2s/step - loss: 0.0341 - accuracy: 0.8412 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 618s 2s/step - loss: 0.0187 - accuracy: 0.8469 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 588s 2s/step - loss: 0.0106 - accuracy: 0.8453 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "160/391 [===========>..................] - ETA: 6:41 - loss: 0.0063 - accuracy: 0.8391"
     ]
    }
   ],
   "source": [
    "# Train the model⏰ This cell may take about a minute to run\n",
    "history1 = model1.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5df71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc39e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can i use this line of code instead and get the same results?\n",
    "\n",
    "# Train the model⏰ This cell may take about a minute to run\n",
    "model1.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e4814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc07c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8adbfa4c",
   "metadata": {},
   "source": [
    "# G. Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b850e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc3287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e130a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55468c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f009357a",
   "metadata": {},
   "source": [
    "# H. Selecting Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
