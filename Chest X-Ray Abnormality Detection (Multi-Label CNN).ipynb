{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "a9f1327e",
=======
   "id": "fe683ab6",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# Chest X-Ray Abnormality Detection (Multi-Label CNN)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "30bd45b9",
=======
   "id": "e8963dfb",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "KENDALL MCNEIL\n",
    "\n",
    "November 2023"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "1eaec37a",
=======
   "id": "b8cf2a8f",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "insert image collage here"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "99e3e60e",
=======
   "id": "15b4ef02",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "DESCRIPTION:  The chest radiograph is one of the most challenging to interpret, which can result in misdiagnosis even for seasoned healthcare providers. Building a strong convolutional neural network (CNN) to detect common thoracic lung diseases in chest x-rays would improve diagnostic accuracy for patients and ultimately save lives through early and accurate detection. The CNN will act as an automated system to support radiologists as a second opinion in reviewing chest x-rays for abnormalities. The work product will alleviate the stress of busy doctors and healthcare providers while also providing patients with a more accurate and efficient diagnosis. \n",
    "\n",
    "OBJECTIVE: The objective, therefore, is to detect a variety (14 total) of common thoracic lung abnormalities in chest x-rays by building a Convolutional Neural Network (CNN) to develop an AI system for thoracic lung abnormality detection. The multi-label neural network model was designed using Tensorflow. \n",
    "\n",
    "AUDIENCES: The general target audience for the project is healthcare providers. The more specific presentation audience is Vingroup Big Data Institute (VinBigData) that is working to build large-scale and high-precision medical imaging solutions based on the latest advancements in AI to facilitate efficient clinical workflows. \n",
    "\n",
    "DATA: VinBigData has provided a dataset of 18,000 CXR scans dicom images labeled by a panel of experienced radiologists for the presence of 14 common thoracic abnormalities: aortic enlargement, atelectasis, calcification, cardiomegaly, consolidation, ILD, infiltration, lung opacity, nodule/mass, other lesion, pleural effusion, pleural thickening, pneumothorax, and pulmonary fibrosis. The dataset was created by assembling de-identified chest X-ray studies provided by two hospitals in Vietnam: the Hospital 108 and the Hanoi Medical University Hospital."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "41b16050",
=======
   "id": "9fd3f7e4",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# A. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12a8edec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74f786c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jackson\\Documents\\flatiron\\CAPSTONE\\data\n"
     ]
    }
   ],
   "source": [
    "cd Documents\\flatiron\\CAPSTONE\\data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 140,
   "id": "94b18d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m categorical_crossentropy\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#additional plotting imports\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers'"
     ]
    }
   ],
=======
   "execution_count": 85,
   "id": "94b18d4a",
   "metadata": {},
   "outputs": [],
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "source": [
    "#basic imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "#tensorflow imports for CNN image classification project \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import regularizers, optimizers\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
<<<<<<< HEAD
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#additional plotting imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
=======
    "\n",
    "#additional plotting imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "9042e966",
=======
   "id": "69b0aabd",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "First, let's read in the data. Then let's store the full dataset under \"original df\" then create a dataframe with only image and class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "13612a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('train.csv')\n",
    "df = original_df[['image_id','class_id']]"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "2e6605ac",
=======
   "id": "30e36f07",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# B. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ae40a4e6",
=======
   "id": "93556a1e",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "Let's create a legend for the class names and class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0090750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Class_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Aortic enlargement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Atelectasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Calcification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Consolidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ILD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Infiltration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Lung Opacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nodule/Mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Other lesion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Pleural effusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Pleural thickening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Pneumothorax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Pulmonary fibrosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>No finding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number          Class_Name\n",
       "0        0  Aortic enlargement\n",
       "1        1         Atelectasis\n",
       "2        2       Calcification\n",
       "3        3        Cardiomegaly\n",
       "4        4       Consolidation\n",
       "5        5                 ILD\n",
       "6        6        Infiltration\n",
       "7        7        Lung Opacity\n",
       "8        8         Nodule/Mass\n",
       "9        9        Other lesion\n",
       "10      10    Pleural effusion\n",
       "11      11  Pleural thickening\n",
       "12      12        Pneumothorax\n",
       "13      13  Pulmonary fibrosis\n",
       "14      14          No finding"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df = pd.DataFrame({'Number': list(range(15)),\n",
    "    'Class_Name': ['Aortic enlargement','Atelectasis','Calcification','Cardiomegaly','Consolidation','ILD',\n",
    "    'Infiltration','Lung Opacity','Nodule/Mass','Other lesion','Pleural effusion','Pleural thickening','Pneumothorax',\n",
    "    'Pulmonary fibrosis','No finding']})\n",
    "class_df"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "aff6769c",
=======
   "id": "c43d186c",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "How many of each class are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
<<<<<<< HEAD
   "id": "088ece75",
=======
   "id": "0cbc0b62",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    31818\n",
       "0      7162\n",
       "3      5427\n",
       "11     4842\n",
       "13     4655\n",
       "8      2580\n",
       "7      2483\n",
       "10     2476\n",
       "9      2203\n",
       "6      1247\n",
       "5      1000\n",
       "2       960\n",
       "4       556\n",
       "1       279\n",
       "12      226\n",
       "Name: class_id, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "class_counts_df = original_df['class_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce17c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "sns.barplot(x=class_counts_df.index, y=class_counts_df.)\n",
    "ax.set_title('Class Distribution for Chest X Ray Image Dataset')\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Count');"
=======
    "class_counts_df = df['class_id'].value_counts()\n",
    "class_counts_df"
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "666eaee7",
=======
   "id": "01186d87",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "The distribution looks okay. There may be a class imbalance that may rear its head later. Let's peak into image ID value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
<<<<<<< HEAD
   "id": "45d3ae8b",
=======
   "id": "531d05af",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "03e6ecfa6f6fb33dfeac6ca4f9b459c9    57\n",
       "fa109c087e46fe1ea27e48ce6d154d2f    52\n",
       "e31be972e181987a8600a8700c1ebe88    48\n",
       "6d5acf3f8a973a26844d617fffe72998    46\n",
       "3a302fbbbf3364aa1a7731b59e6b98ec    46\n",
       "Name: image_id, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ae0c2d6e",
=======
   "id": "f63d343f",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "That's odd. Some image IDs have over 50 counts. Let's check on duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
<<<<<<< HEAD
   "id": "d869fb7d",
=======
   "id": "6cbea06e",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41943"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "b084d068",
=======
   "id": "1ea25f94",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "Wow. There are a lot of duplicates. Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
<<<<<<< HEAD
   "id": "fbd15ded",
=======
   "id": "7d33aef0",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8573fa95ec3defbe2dec45d85a5093a1    10\n",
       "7d0e636b3ef2ccbb0c67b3243a1478ce    10\n",
       "7e9efb8ee0bae7af280f5ea091f8d245    10\n",
       "0c6a7e3c733bd4f4d89443ca16615fc6    10\n",
       "9e9cc9d084546cb6d7f7ddba63411a81     9\n",
       "c42997b00e59f4523788aa9fbe1f7526     9\n",
       "e9954e6e3b2d0c5bf990a519c0ba5abe     9\n",
       "54e6184c63c75a9695d7effc17969ad0     9\n",
       "a4fc9faa46af26c5fc462772d88d0af3     9\n",
       "d0043062cebce85f2487407bc033d405     9\n",
       "8ac12a69ad57ca73535e04b6cfba5edb     9\n",
       "2e285b95faad220e17e6cbfbe514733e     9\n",
       "26585204e3c296a3b105bd5bd1c537ee     9\n",
       "ae3382840414ce4de46c3827674b9709     9\n",
       "5673fae597c1b5218f79eead1f413da6     9\n",
       "54f9194379210945be0ed72fac357456     9\n",
       "6e4915e84e38de5d90efe0ecfd8f2cf6     9\n",
       "734bbd50e6a2265ae0092510852c9c24     9\n",
       "41cee81dbe9886a56b6c3ee56fb23448     9\n",
       "eca27ff9495044fbcd347ee51a8f1187     9\n",
       "Name: image_id, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df['image_id'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ff8dbcf2",
=======
   "id": "20031ac3",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "Much better. Let's also check that there are no duplicate photos in the images folder just to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ac311ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'images'\n",
    "\n",
<<<<<<< HEAD
    "#create a dictionary to store encountered file names.\n",
    "file_names = {}\n",
    "\n",
    "#iterate through the files in the folder.\n",
    "for filename in os.listdir(folder_path):\n",
    "    #check if the file is a regular file (not a subdirectory).\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        #check if the file name has been encountered before.\n",
=======
    "# Create a dictionary to store encountered file names.\n",
    "file_names = {}\n",
    "\n",
    "# Iterate through the files in the folder.\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is a regular file (not a subdirectory).\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        # Check if the file name has been encountered before.\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "        if filename in file_names:\n",
    "            print(f'Duplicate file name: {filename}')\n",
    "            print(f'First occurrence: {file_names[filename]}')\n",
    "            print(f'Second occurrence: {os.path.join(folder_path, filename)}')\n",
    "        else:\n",
<<<<<<< HEAD
    "            #store the file name and its full path for future reference.\n",
=======
    "            # Store the file name and its full path for future reference.\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "            file_names[filename] = os.path.join(folder_path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9838cb",
   "metadata": {},
   "source": [
    "Great. There are no duplicate photos."
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "51566657",
=======
   "id": "8020870d",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# C. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "cb87af4c",
=======
   "id": "758a26f4",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "The next few cells are outdated as I was working to organize the data. I created a function that would create class id folders and copy images respectively into them, but then realized there are more efficient and storage-conscious ways to organize the data.\n",
    "\n",
    "Additionally, it is worth mentioning that the data provided did not have labeled targets (likely because it is an active competition). Therefore, we will use the 15,000 images in the train folder and take a subset of that set for testing. We will use 12500 images for training and 2500 images for testing - a 17/83% split. \n",
    "\n",
    "Originally, I had random.sample randomly select 2500 images and move them into the test folder, but later on decided to perform the split inside our ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
<<<<<<< HEAD
   "id": "8d3a4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create class folders and copy images\n",
=======
   "id": "2ad65975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create class folders and copy images\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "def create_class_folders_and_copy_images(base_folder, class_counts, df):\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row['image_id']\n",
    "        class_id = row['class_id']\n",
    "\n",
<<<<<<< HEAD
    "        #create a folder for the class if it doesn't exist\n",
=======
    "        # Create a folder for the class if it doesn't exist\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "        class_folder = os.path.join(base_folder, str(class_id))\n",
    "        os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "        source_image_path = os.path.join(base_folder, f\"{image_id}.PNG\")\n",
    "        image_path = os.path.join(class_folder, f\"{image_id}.PNG\")\n",
    "\n",
<<<<<<< HEAD
    "        #check if the source file exists before copying\n",
=======
    "        # Check if the source file exists before copying\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "        if os.path.exists(source_image_path):\n",
    "            shutil.copy(source_image_path, image_path)\n",
    "            print(f\"Copied: {source_image_path} -> {image_path}\")\n",
    "        else:\n",
    "            print(f\"Source file not found: {source_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
<<<<<<< HEAD
   "id": "454209c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class folders and copy images for the \"train\" folder\n",
    "#create_class_folders_and_copy_images(train_folder, class_counts, df)\n",
    "\n",
    "#create class folders and copy images for the \"test\" folder\n",
=======
   "id": "29148fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class folders and copy images for the \"train\" folder\n",
    "#create_class_folders_and_copy_images(train_folder, class_counts, df)\n",
    "\n",
    "# Create class folders and copy images for the \"test\" folder\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "#create_class_folders_and_copy_images(test_folder, class_counts, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e5f4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_folder = 'train'\n",
    "#test_folder = 'test'\n",
    "\n",
<<<<<<< HEAD
    "#list all image files in the train folder\n",
    "#image_files = [file for file in os.listdir(train_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "#randomly select 2500 images from the list\n",
    "#selected_images = random.sample(image_files, 2500)\n",
    "\n",
    "#move the selected images to the test folder\n",
=======
    "# List all image files in the train folder\n",
    "#image_files = [file for file in os.listdir(train_folder) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Randomly select 2500 images from the list\n",
    "#selected_images = random.sample(image_files, 2500)\n",
    "\n",
    "# Move the selected images to the test folder\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "#for image in selected_images:\n",
    "    #source_path = os.path.join(train_folder, image)\n",
    "    #destination_path = os.path.join(test_folder, image)\n",
    "    #shutil.move(source_path, destination_path)\n",
    "\n",
    "#print(\"Randomly selected and moved 2500 images to the test folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "83870a8a",
=======
   "id": "fa80020b",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "We need to perform additional data preprocessing before using Tensorflow. Currently, there are multiple rows for each image that falls into multiple classes. To fix this for our ImageDataGenerator, we need to groupby class ID, one hot encode the class IDs, and then merge the dataframes. That will create a final dataframe that has one row per image with one hot encoded classes. Let's also shuffle the dataframe images just in case because I will be selecting the first 12500 for the test set and the remaining 2500 for the test set. Last, let's add \".png\" to all the image IDs so that our ImageDataGenerator can locate them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "64b8041a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>11.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>13.0</th>\n",
       "      <th>14.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>887192b8e51bd077f02cf51c57f59584.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52ed2f5f58b7b0547c37a78e735b4e4f.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>770959dcc1087ec6ed7c307e6496aa65.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cb30e92272f713dd8224320c17cfedbf.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3349c0d8861ff59db82c4e6f1d10705a.png</td>\n",
       "      <td>[11, 0, 10, 9, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>dba73ad098671788c3ed72fc9b07bdd3.png</td>\n",
       "      <td>[3, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>2a3def0aa2b27bea4235348e5d4cf345.png</td>\n",
       "      <td>[3, 0, 13, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>8bd12223f6668343caadd2fcb79b8d56.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>c79a0c426c686a8ba6bcefd9ef058886.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>69e4c9580ef08026363cb7ce6e6f7022.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   image_id           class_id  0.0  1.0  2.0  \\\n",
       "0      887192b8e51bd077f02cf51c57f59584.png               [14]    0    0    0   \n",
       "1      52ed2f5f58b7b0547c37a78e735b4e4f.png               [14]    0    0    0   \n",
       "2      770959dcc1087ec6ed7c307e6496aa65.png               [14]    0    0    0   \n",
       "3      cb30e92272f713dd8224320c17cfedbf.png               [14]    0    0    0   \n",
       "4      3349c0d8861ff59db82c4e6f1d10705a.png  [11, 0, 10, 9, 3]    1    0    0   \n",
       "...                                     ...                ...  ...  ...  ...   \n",
       "14995  dba73ad098671788c3ed72fc9b07bdd3.png             [3, 0]    1    0    0   \n",
       "14996  2a3def0aa2b27bea4235348e5d4cf345.png      [3, 0, 13, 6]    1    0    0   \n",
       "14997  8bd12223f6668343caadd2fcb79b8d56.png               [14]    0    0    0   \n",
       "14998  c79a0c426c686a8ba6bcefd9ef058886.png               [14]    0    0    0   \n",
       "14999  69e4c9580ef08026363cb7ce6e6f7022.png               [14]    0    0    0   \n",
       "\n",
       "       3.0  4.0  5.0  6.0  7.0  8.0  9.0  10.0  11.0  12.0  13.0  14.0  \n",
       "0        0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "1        0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "2        0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "3        0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "4        1    0    0    0    0    0    1     1     1     0     0     0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...  \n",
       "14995    1    0    0    0    0    0    0     0     0     0     0     0  \n",
       "14996    1    0    0    1    0    0    0     0     0     0     1     0  \n",
       "14997    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "14998    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "14999    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "\n",
       "[15000 rows x 17 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: Group by image ID\n",
    "grouped = df.groupby('image_id')['class_id'].apply(list).reset_index()\n",
    "\n",
    "#Step 2: Perform one-hot encoding\n",
    "one_hot_encoded = pd.get_dummies(grouped['class_id'].apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "#Step 3: Merge dataframes \n",
    "final_df = grouped.merge(one_hot_encoded, left_index=True, right_index=True)\n",
    "\n",
    "#Step 4: shuffle the dataframe\n",
    "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Step 5: add \".png\" to the image_ids \n",
    "final_df['image_id'] = final_df['image_id'] + '.png'\n",
    "\n",
    "#The final_df DataFrame now contains one-hot encoded class labels for each image ID with one row per image\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "a34fe5a5",
=======
   "id": "2e610672",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "Now we're cookin' and ready for Tensorflow's ImageDataGenerator!"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "8cf55d84",
=======
   "id": "27a8e97a",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# D. ImageDataGenerator Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
<<<<<<< HEAD
   "id": "63f90775",
=======
   "id": "0f7f1930",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>11.0</th>\n",
       "      <th>12.0</th>\n",
       "      <th>13.0</th>\n",
       "      <th>14.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>41a95ec8dafbc08bf62528825bff13c5.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>4f09499741de881b71bb549e9cacdd66.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>dfc80884bf97cb71d4848b501cb7f702.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>59d6e0d02e892e31e540a05c78338f53.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>9ef2c06481a9e65d71e6743d04217462.png</td>\n",
       "      <td>[10, 7, 4, 11]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>dba73ad098671788c3ed72fc9b07bdd3.png</td>\n",
       "      <td>[3, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>2a3def0aa2b27bea4235348e5d4cf345.png</td>\n",
       "      <td>[3, 0, 13, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>8bd12223f6668343caadd2fcb79b8d56.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>c79a0c426c686a8ba6bcefd9ef058886.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>69e4c9580ef08026363cb7ce6e6f7022.png</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   image_id        class_id  0.0  1.0  2.0  \\\n",
       "12500  41a95ec8dafbc08bf62528825bff13c5.png            [14]    0    0    0   \n",
       "12501  4f09499741de881b71bb549e9cacdd66.png            [14]    0    0    0   \n",
       "12502  dfc80884bf97cb71d4848b501cb7f702.png            [14]    0    0    0   \n",
       "12503  59d6e0d02e892e31e540a05c78338f53.png            [14]    0    0    0   \n",
       "12504  9ef2c06481a9e65d71e6743d04217462.png  [10, 7, 4, 11]    0    0    0   \n",
       "...                                     ...             ...  ...  ...  ...   \n",
       "14995  dba73ad098671788c3ed72fc9b07bdd3.png          [3, 0]    1    0    0   \n",
       "14996  2a3def0aa2b27bea4235348e5d4cf345.png   [3, 0, 13, 6]    1    0    0   \n",
       "14997  8bd12223f6668343caadd2fcb79b8d56.png            [14]    0    0    0   \n",
       "14998  c79a0c426c686a8ba6bcefd9ef058886.png            [14]    0    0    0   \n",
       "14999  69e4c9580ef08026363cb7ce6e6f7022.png            [14]    0    0    0   \n",
       "\n",
       "       3.0  4.0  5.0  6.0  7.0  8.0  9.0  10.0  11.0  12.0  13.0  14.0  \n",
       "12500    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "12501    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "12502    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "12503    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "12504    0    1    0    0    1    0    0     1     1     0     0     0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...  \n",
       "14995    1    0    0    0    0    0    0     0     0     0     0     0  \n",
       "14996    1    0    0    1    0    0    0     0     0     0     1     0  \n",
       "14997    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "14998    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "14999    0    0    0    0    0    0    0     0     0     0     0     1  \n",
       "\n",
       "[2500 rows x 17 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[12500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5b7c6ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 validated image filenames belonging to 15 classes.\n",
      "Found 2500 validated image filenames belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen= ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen= ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "final_columns = final_df.columns[2:].tolist()\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=final_df[:12500], directory='images', x_col='image_id',\n",
    "    y_col='class_id', seed=42, class_mode='categorical', color_mode='grayscale')\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "    dataframe=final_df[12500:], directory='images', x_col='image_id', \n",
    "    y_col='class_id', seed=42, class_mode='categorical', batch_size=20, color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f49e1e37",
=======
   "id": "65315ddd",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "Let's also create a function to evaluate the models moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
<<<<<<< HEAD
   "id": "df8ac9e7",
=======
   "id": "44341f4f",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    train_loss, train_accuracy = model.evaluate(train_generator)\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss}')\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    \n",
    "    print('----------')\n",
    "    \n",
    "    \n",
    "    print(f'Train Accuracy: {train_accuracy}')\n",
    "    print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "dbb1d8ad",
=======
   "id": "4a984358",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# E. First Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
<<<<<<< HEAD
   "id": "e6f018a0",
=======
   "id": "14ff09bd",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4194368   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4195343 (16.00 MB)\n",
      "Trainable params: 4195343 (16.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#define the first simple model\n",
    "fsmodel = keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 1)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(15, activation='sigmoid')  #we will use sigmoid because this is a multi-label problem\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "fsmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#display the model summary\n",
=======
    "# Define the first simple model\n",
    "fsmodel = keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 1)),  # Adjust input shape as needed\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(15, activation='sigmoid')  # Use 'sigmoid' for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "fsmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "fsmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 126,
   "id": "7e8857f0",
=======
   "execution_count": null,
   "id": "c71fc04e",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 84s 212ms/step - loss: 0.4432 - accuracy: 0.6565\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.2321 - accuracy: 0.7092\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 0.2313 - accuracy: 0.7150\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 85s 218ms/step - loss: 0.2280 - accuracy: 0.7118\n",
      "Epoch 5/10\n",
<<<<<<< HEAD
      "391/391 [==============================] - 84s 214ms/step - loss: 0.2167 - accuracy: 0.7620\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 88s 224ms/step - loss: 0.2067 - accuracy: 0.7853\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 82s 209ms/step - loss: 0.2001 - accuracy: 0.7926\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 83s 214ms/step - loss: 0.1912 - accuracy: 0.8050\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 88s 225ms/step - loss: 0.1907 - accuracy: 0.8047\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 86s 219ms/step - loss: 0.1878 - accuracy: 0.8102\n",
      "391/391 [==============================] - 67s 170ms/step - loss: 0.1831 - accuracy: 0.8167\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.1878 - accuracy: 0.8088\n",
      "Train Loss: 0.1830577701330185\n",
      "Test Loss: 0.18780778348445892\n",
      "----------\n",
      "Train Accuracy: 0.8167200088500977\n",
      "Test Accuracy: 0.8087999820709229\n"
=======
      "105/391 [=======>......................] - ETA: 55s - loss: 0.2107 - accuracy: 0.7568"
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#train the model⏰ this cell may take about a minute to run\n",
    "fsmodel.fit(train_generator, epochs=10, batch_size=64)\n",
    "\n",
    "#evaluate the model\n",
=======
    "# Train the model⏰ This cell may take about a minute to run\n",
    "fsmhistory = fsmodel.fit(train_generator, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "evaluate(fsmodel)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "51ddc61d",
   "metadata": {},
   "source": [
    "Not bad - 82% training accuracy and 81% test accuracy to start. Not a high variance and it looks like the model is not overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f846b87",
=======
   "id": "b8932ac5",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# F. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "526f1e2b",
=======
   "id": "d1cfc67a",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "This is our baseline first simple model that our model must perform better than. Let's start by adding some Conv2D and MaxPooling2D layers."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 128,
   "id": "6061ad5e",
=======
   "execution_count": 113,
   "id": "92b255fe",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 127, 127, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 246016)            0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               31490176  \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 15)                1935      \n",
=======
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 127, 127, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 62, 62, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 246016)            0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               31490176  \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 15)                1935      \n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31510927 (120.20 MB)\n",
      "Trainable params: 31510927 (120.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
<<<<<<< HEAD
    "    layers.Dense(15, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#display the model summary\n",
=======
    "    layers.Dense(15, activation='sigmoid')  # Use 'sigmoid' activation for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 133,
   "id": "9f82a6b2",
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 115,
   "id": "a83ca8fd",
   "metadata": {},
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/30\n",
      "391/391 [==============================] - 505s 1s/step - loss: 0.0036 - accuracy: 0.8370\n",
      "Epoch 2/30\n",
      "391/391 [==============================] - 499s 1s/step - loss: 0.0040 - accuracy: 0.8368\n",
      "Epoch 3/30\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.0049 - accuracy: 0.8352\n",
      "Epoch 4/30\n",
      "391/391 [==============================] - 512s 1s/step - loss: 0.0047 - accuracy: 0.8395\n",
      "Epoch 5/30\n",
      "391/391 [==============================] - 504s 1s/step - loss: 0.0037 - accuracy: 0.8322\n",
      "Epoch 6/30\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.0030 - accuracy: 0.8370\n",
      "Epoch 7/30\n",
      "391/391 [==============================] - 487s 1s/step - loss: 0.0020 - accuracy: 0.8356\n",
      "Epoch 8/30\n",
      "391/391 [==============================] - 497s 1s/step - loss: 0.0035 - accuracy: 0.8317\n",
      "Epoch 9/30\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.0033 - accuracy: 0.8294\n",
      "Epoch 10/30\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.0030 - accuracy: 0.8343\n",
      "Epoch 11/30\n",
      "391/391 [==============================] - 483s 1s/step - loss: 0.0022 - accuracy: 0.8341\n",
      "Epoch 12/30\n",
      "391/391 [==============================] - 1343s 3s/step - loss: 0.0038 - accuracy: 0.8341\n",
      "Epoch 13/30\n",
      "391/391 [==============================] - 512s 1s/step - loss: 0.0024 - accuracy: 0.8361\n",
      "Epoch 14/30\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.0015 - accuracy: 0.8358\n",
      "Epoch 15/30\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.0016 - accuracy: 0.8312\n",
      "Epoch 16/30\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.0019 - accuracy: 0.8293\n",
      "Epoch 17/30\n",
      "391/391 [==============================] - 508s 1s/step - loss: 0.0028 - accuracy: 0.8256\n",
      "Epoch 18/30\n",
      "391/391 [==============================] - 544s 1s/step - loss: 0.0039 - accuracy: 0.8311\n",
      "Epoch 19/30\n",
      "391/391 [==============================] - 509s 1s/step - loss: 0.0025 - accuracy: 0.8282\n",
      "Epoch 20/30\n",
      "391/391 [==============================] - 484s 1s/step - loss: 0.0017 - accuracy: 0.8216\n",
      "Epoch 21/30\n",
      "391/391 [==============================] - 496s 1s/step - loss: 0.0014 - accuracy: 0.8324\n",
      "Epoch 22/30\n",
      "391/391 [==============================] - 523s 1s/step - loss: 0.0027 - accuracy: 0.8284\n",
      "Epoch 23/30\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.0035 - accuracy: 0.8246\n",
      "Epoch 24/30\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.0018 - accuracy: 0.8290\n",
      "Epoch 25/30\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.0012 - accuracy: 0.8255\n",
      "Epoch 26/30\n",
      "391/391 [==============================] - 500s 1s/step - loss: 0.0013 - accuracy: 0.8273\n",
      "Epoch 27/30\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.0013 - accuracy: 0.8266\n",
      "Epoch 28/30\n",
      "391/391 [==============================] - 500s 1s/step - loss: 9.4341e-04 - accuracy: 0.8268\n",
      "Epoch 29/30\n",
      "391/391 [==============================] - 500s 1s/step - loss: 6.4128e-04 - accuracy: 0.8244\n",
      "Epoch 30/30\n",
      "391/391 [==============================] - 501s 1s/step - loss: 0.0016 - accuracy: 0.8294\n",
      "391/391 [==============================] - 146s 373ms/step - loss: 0.0019 - accuracy: 0.8127\n",
      "125/125 [==============================] - 40s 321ms/step - loss: 1.0293 - accuracy: 0.7188\n",
      "Train Loss: 0.001919804373756051\n",
      "Test Loss: 1.0293068885803223\n",
      "----------\n",
      "Train Accuracy: 0.812720000743866\n",
      "Test Accuracy: 0.7188000082969666\n"
=======
      "Epoch 1/10\n",
      "391/391 [==============================] - 798s 2s/step - loss: 0.2078 - accuracy: 0.7598\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 633s 2s/step - loss: 0.1643 - accuracy: 0.7909\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 550s 1s/step - loss: 0.1405 - accuracy: 0.8046\n",
      "Epoch 4/10\n",
      " 28/391 [=>............................] - ETA: 8:35 - loss: 0.1112 - accuracy: 0.8225"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#can i use this line of code instead and get the same results?\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model⏰ This cell may take about a minute to run\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model1\u001b[38;5;241m.\u001b[39mfit(train_generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m evaluate(model1)\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function(\u001b[38;5;241m*\u001b[39margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda\\anaconda-this one\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#train the model⏰ this cell may take about a minute to run\n",
    "model1.fit(train_generator, epochs=30, batch_size=64)\n",
    "\n",
    "#evaluate the model\n",
=======
    "# Train the model⏰ This cell may take about a minute to run\n",
    "model1.fit(train_generator, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "evaluate(model1)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "195f009b",
   "metadata": {},
   "source": [
    "Let's add dropout and see how the model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2b1ff560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 254, 254, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 127, 127, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 62, 62, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 246016)            0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               31490176  \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 15)                1935      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31510927 (120.20 MB)\n",
      "Trainable params: 31510927 (120.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(15, activation='sigmoid')\n",
    "])\n",
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacfc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(100,100,3)))\n",
    "\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv2D(32, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(512))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
    "\n",
    "#compile the model \n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#display the model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 136,
   "id": "be356714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.2146 - accuracy: 0.7468\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 2035s 5s/step - loss: 0.1657 - accuracy: 0.7965\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 474s 1s/step - loss: 0.1446 - accuracy: 0.7954\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 484s 1s/step - loss: 0.1182 - accuracy: 0.8069\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 494s 1s/step - loss: 0.0888 - accuracy: 0.8201\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.0578 - accuracy: 0.8279\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 492s 1s/step - loss: 0.0350 - accuracy: 0.8367\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 491s 1s/step - loss: 0.0214 - accuracy: 0.8275\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 488s 1s/step - loss: 0.0135 - accuracy: 0.8317\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 4027s 10s/step - loss: 0.0103 - accuracy: 0.8285\n",
      "391/391 [==============================] - 155s 396ms/step - loss: 0.0060 - accuracy: 0.8528\n",
      "125/125 [==============================] - 54s 434ms/step - loss: 0.5242 - accuracy: 0.7516\n",
      "Train Loss: 0.006017238367348909\n",
      "Test Loss: 0.5241689682006836\n",
      "----------\n",
      "Train Accuracy: 0.8528000116348267\n",
      "Test Accuracy: 0.7516000270843506\n"
     ]
    }
   ],
   "source": [
    "#train the model⏰ this cell may take about a minute to run\n",
    "model2.fit(train_generator, epochs=10, batch_size=64)\n",
    "\n",
    "#evaluate the model\n",
    "evaluate(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0a2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that creates your Keras model\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(256, 256, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(15, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#wrap the model using KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "#defining hyperparameters to search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout_rate': [0.2, 0.4, 0.6]\n",
    "}\n",
    "\n",
    "#creating and fitting the GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_result = grid.fit(train_generator, y=None, epochs=10, batch_size=64, verbose=2)\n",
    "\n",
    "#spitting out results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7a546",
=======
   "execution_count": null,
   "id": "e2a95dd5",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "27c192f7",
=======
   "id": "e469e276",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# G. Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "22dc660a",
=======
   "id": "c77c8393",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "20683b94",
=======
   "id": "df36cb46",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "6535e9ae",
=======
   "id": "6fcbebdf",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "e0b42780",
=======
   "id": "b993af06",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "41559bb5",
=======
   "id": "dab3114e",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "source": [
    "# H. Selecting Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "4b657ac7",
=======
   "id": "63f983cf",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "24a6bad0",
=======
   "id": "553a8923",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "0d4bbf71",
=======
   "id": "c304c6ef",
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "810917ca",
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "id": "5b746ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "010edeb4",
   "metadata": {},
   "source": [
    "# ATTEMPT #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f14343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_iter = ImageDataGenerator().flow_from_dataframe(\n",
    "    img_metadata_df,\n",
    "    directory='/home/rodrigo/.keras/datasets',\n",
    "    x_col='filename',\n",
    "    y_col='labels',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95159f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen= ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen= ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "final_columns = final_df.columns[2:].tolist()\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=final_df[:12500], directory='images', x_col='image_id',\n",
    "    y_col=final_columns, seed=42, class_mode='raw', color_mode='grayscale')\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "    dataframe=final_df[12500:], directory='images', x_col='image_id', batch_size=1, shuffle=False,\n",
    "    seed=42, class_mode=None, color_mode='grayscale')"
   ]
>>>>>>> e3878e4e023ebb48dd461cc31f0c21aba29f8883
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
